{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92359c70-7011-4a4c-8fc6-a1f766657d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/07 16:48:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/07 16:48:44 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|     pickup_datetime|    dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+--------------------+--------------------+------------+------------+-------+----------------------+\n",
      "|              B00001|03/01/2023 12:24:...|03/01/2023 02:35:...|        NULL|        NULL|   NULL|                B00001|\n",
      "|              B00008|01/01/2023 12:30:...|01/01/2023 01:00:...|        NULL|        NULL|   NULL|                B00008|\n",
      "|              B00078|01/01/2023 12:01:...|01/01/2023 03:15:...|        NULL|        NULL|   NULL|                B00078|\n",
      "|              B00111|01/01/2023 12:30:...|01/01/2023 01:05:...|        NULL|        NULL|   NULL|                B03406|\n",
      "|              B00112|01/01/2023 12:34:...|01/01/2023 12:52:...|        NULL|          14|   NULL|                B00112|\n",
      "+--------------------+--------------------+--------------------+------------+------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO Test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\n",
    "    \"s3a://trabalho/2023_For_Hire_Vehicles_Trip_Data_20251206.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ada0523-92da-41d9-bf3c-2fac705ec607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- pu_location_id: integer (nullable = true)\n",
      " |-- do_location_id: integer (nullable = true)\n",
      " |-- sr_flag: string (nullable = true)\n",
      " |-- affiliated_base_number: string (nullable = true)\n",
      " |-- trip_duration_min: double (nullable = true)\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- pickup_year: integer (nullable = true)\n",
      " |-- pickup_month: integer (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- missing_pu_location: boolean (nullable = false)\n",
      " |-- missing_do_location: boolean (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------+--------------+-------+----------------------+------------------+-----------+-----------+------------+-----------+-------------------+-------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|pu_location_id|do_location_id|sr_flag|affiliated_base_number| trip_duration_min|pickup_date|pickup_year|pickup_month|pickup_hour|missing_pu_location|missing_do_location|\n",
      "+--------------------+-------------------+-------------------+--------------+--------------+-------+----------------------+------------------+-----------+-----------+------------+-----------+-------------------+-------------------+\n",
      "|              B00856|2023-01-01 00:53:48|2023-01-01 01:02:40|          NULL|            76|   NULL|                B03436| 8.866666666666667| 2023-01-01|       2023|           1|          0|               true|              false|\n",
      "|              B01843|2023-01-01 00:17:00|2023-01-01 00:27:00|          NULL|          NULL|   NULL|                  NULL|              10.0| 2023-01-01|       2023|           1|          0|               true|               true|\n",
      "|              B03284|2023-01-01 00:55:30|2023-01-01 01:27:07|            63|           146|   NULL|                  NULL|31.616666666666667| 2023-01-01|       2023|           1|          0|              false|              false|\n",
      "|              B00856|2023-01-01 01:07:42|2023-01-01 01:44:59|          NULL|            81|   NULL|                B03404| 37.28333333333333| 2023-01-01|       2023|           1|          1|               true|              false|\n",
      "|              B00856|2023-01-01 01:25:20|2023-01-01 01:45:19|          NULL|            77|   NULL|                B03404|19.983333333333334| 2023-01-01|       2023|           1|          1|               true|              false|\n",
      "+--------------------+-------------------+-------------------+--------------+--------------+-------+----------------------+------------------+-----------+-----------+------------+-----------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType, TimestampType\n",
    "\n",
    "# ================================================\n",
    "# 1. Padronizar nomes das colunas (snake_case)\n",
    "# ================================================\n",
    "df_clean = df.select(\n",
    "    F.col(\"dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "    F.col(\"pickup_datetime\").alias(\"pickup_datetime\"),\n",
    "    F.col(\"dropOff_datetime\").alias(\"dropoff_datetime\"),\n",
    "    F.col(\"PUlocationID\").alias(\"pu_location_id\"),\n",
    "    F.col(\"DOlocationID\").alias(\"do_location_id\"),\n",
    "    F.col(\"SR_Flag\").alias(\"sr_flag\"),\n",
    "    F.col(\"Affiliated_base_number\").alias(\"affiliated_base_number\")\n",
    ")\n",
    "\n",
    "# ================================================\n",
    "# 2. Converter tipos corretamente\n",
    "# ================================================\n",
    "\n",
    "df_clean = (\n",
    "    df_clean\n",
    "    .withColumn(\"pickup_datetime\",\n",
    "                F.to_timestamp(\"pickup_datetime\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "    .withColumn(\"dropoff_datetime\",\n",
    "                F.to_timestamp(\"dropoff_datetime\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    ")\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# 3. Criar novas features úteis\n",
    "# ================================================\n",
    "\n",
    "df_clean = (\n",
    "    df_clean\n",
    "    # duração da viagem (minutos)\n",
    "    .withColumn(\n",
    "        \"trip_duration_min\",\n",
    "        (F.unix_timestamp(\"dropoff_datetime\") - \n",
    "         F.unix_timestamp(\"pickup_datetime\")) / 60\n",
    "    )\n",
    "    # Ano / mês / dia / hora\n",
    "    .withColumn(\"pickup_date\", F.to_date(\"pickup_datetime\"))\n",
    "    .withColumn(\"pickup_year\", F.year(\"pickup_datetime\"))\n",
    "    .withColumn(\"pickup_month\", F.month(\"pickup_datetime\"))\n",
    "    .withColumn(\"pickup_hour\", F.hour(\"pickup_datetime\"))\n",
    "    # Indicador se a viagem teve zona de origem ou destino faltando\n",
    "    .withColumn(\"missing_pu_location\", F.col(\"pu_location_id\").isNull())\n",
    "    .withColumn(\"missing_do_location\", F.col(\"do_location_id\").isNull())\n",
    ")\n",
    "\n",
    "# ================================================\n",
    "# 4. Remover registros inconsistentes\n",
    "# ================================================\n",
    "df_clean = df_clean.filter(\n",
    "    (F.col(\"dropoff_datetime\") > F.col(\"pickup_datetime\")) & \n",
    "    (F.col(\"trip_duration_min\") < 720)  # 12h limite\n",
    ")\n",
    "\n",
    "# ================================================\n",
    "# 5. Remover duplicatas\n",
    "# ================================================\n",
    "df_clean = df_clean.dropDuplicates()\n",
    "\n",
    "# ================================================\n",
    "# 6. Resultado final\n",
    "# ================================================\n",
    "df_clean.printSchema()\n",
    "df_clean.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "617ab385-4238-48ed-a486-996dae590021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_clean.write.mode(\"overwrite\").parquet(\n",
    "    \"s3a://trabalho/limpo/for_hire_2023/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0998e0a-505b-42ba-982b-2c11c19af05e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
